{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "ORIG_BASE = \"../input\"\n",
    "DEST_BASE = \"../processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando: train\n",
      "Processando: valid\n",
      "Processando: test\n"
     ]
    }
   ],
   "source": [
    "def process_images(subdir, is_test=False):\n",
    "    input_dir = Path(ORIG_BASE) / subdir\n",
    "    output_dir = Path(DEST_BASE) / subdir\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if is_test:\n",
    "        for img_path in input_dir.glob(\"*.*\"):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = img.resize(IMG_SIZE, Image.LANCZOS)\n",
    "                dest_path = output_dir / img_path.name\n",
    "                img.save(dest_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {img_path}: {e}\")\n",
    "    else:\n",
    "        for class_folder in input_dir.iterdir():\n",
    "            if class_folder.is_dir():\n",
    "                output_class_dir = output_dir / class_folder.name\n",
    "                output_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                for img_path in class_folder.glob(\"*.jpg\"):\n",
    "                    try:\n",
    "                        img = Image.open(img_path).convert(\"RGB\")\n",
    "                        img = img.resize(IMG_SIZE, Image.LANCZOS)\n",
    "                        dest_path = output_class_dir / img_path.name\n",
    "                        img.save(dest_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erro ao processar {img_path}: {e}\")\n",
    "\n",
    "for subset in [\"train\", \"valid\", \"test\"]:\n",
    "    print(f\"Processando: {subset}\")\n",
    "    is_test = subset == \"test\"\n",
    "    process_images(subset, is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6552 files belonging to 102 classes.\n",
      "(32, 224, 224, 3)\n",
      "Found 818 files belonging to 102 classes.\n",
      "Found 819 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'processed', 'train')\n",
    "VALID_DIR = os.path.join(BASE_DIR, 'processed', 'valid')\n",
    "TEST_DIR = os.path.join(BASE_DIR, 'processed', 'test')\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    seed=123\n",
    ")\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    VALID_DIR,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TEST_DIR,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    seed=123\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.Rescaling(1./255),  # Normaliza as imagens entre 0 e 1\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(102, activation='softmax')  # 102 classes para flores\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Verificar formato do primeiro batch de imagens\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)  # Verifique se a forma é (batch_size, altura, largura, canais)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 670ms/step - accuracy: 0.0396 - loss: 5.4133 - val_accuracy: 0.1100 - val_loss: 3.7986\n",
      "Epoch 2/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 586ms/step - accuracy: 0.1835 - loss: 3.4047 - val_accuracy: 0.2579 - val_loss: 3.0424\n",
      "Epoch 3/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 619ms/step - accuracy: 0.4841 - loss: 2.0179 - val_accuracy: 0.2677 - val_loss: 3.5004\n",
      "Epoch 4/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 597ms/step - accuracy: 0.8236 - loss: 0.6923 - val_accuracy: 0.2848 - val_loss: 4.6935\n",
      "Epoch 5/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 593ms/step - accuracy: 0.9519 - loss: 0.1865 - val_accuracy: 0.2579 - val_loss: 5.0643\n",
      "Epoch 6/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 617ms/step - accuracy: 0.9899 - loss: 0.0536 - val_accuracy: 0.2775 - val_loss: 5.5538\n",
      "Epoch 7/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 661ms/step - accuracy: 0.9942 - loss: 0.0274 - val_accuracy: 0.2738 - val_loss: 5.4949\n",
      "Epoch 8/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 620ms/step - accuracy: 0.9979 - loss: 0.0133 - val_accuracy: 0.2702 - val_loss: 5.9754\n",
      "Epoch 9/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 579ms/step - accuracy: 0.9996 - loss: 0.0039 - val_accuracy: 0.2836 - val_loss: 6.2691\n",
      "Epoch 10/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 585ms/step - accuracy: 1.0000 - loss: 7.7247e-04 - val_accuracy: 0.2958 - val_loss: 6.4454\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo com sucesso!\n",
      "Arquivo do modelo encontrado em: d:\\project_hub\\Flower Recognization\\notebooks\\modelo_flores.keras\n"
     ]
    }
   ],
   "source": [
    "# Salvando o modelo treinado no novo formato\n",
    "model.save('modelo_flores.keras')\n",
    "print(\"Modelo salvo com sucesso!\")\n",
    "\n",
    "import os\n",
    "if os.path.exists('modelo_flores.keras'):\n",
    "    print(f\"Arquivo do modelo encontrado em: {os.path.abspath('modelo_flores.keras')}\")\n",
    "else:\n",
    "    print(\"Erro: Arquivo do modelo não foi criado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
